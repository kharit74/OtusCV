{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание:\n",
    "Собрать и подготовить свой собственный датасет, можно в соответствии с темой будущего проекта.\n",
    "Аугментировать данные техниками из занятия.\n",
    "\n",
    "## Данные:\n",
    "Свой датасет\n",
    "\n",
    "## Задачи:\n",
    "1) Взять предобученную модель. \n",
    "    * Cтандартную предобученную модель из библиотек PyTorch `torchvision.models`\n",
    "    * Примеры: ResNet18, VGG16, EfficientNet.\n",
    "2) Применить несколько стратегий аугментации данных \n",
    "    * Определение стратегий аугментации\n",
    "    * Пример Torchvision: `transforms.RandomHorizontalFlip`, `transforms.ColorJitter`.\n",
    "    * Пример Albumentations: `HorizontalFlip`, `RandomBrightnessContrast`, `Cutout`, `ShiftScaleRotate`.\n",
    "    * Пример сложных: <i>MixUp</i>, <i>CutMix</i>, <i>Mosaic</i>, <i>Weather</i>, <i>Style Transfer</i>.\n",
    "3) Собрать результаты успешности стратегий в сводную табличку.\n",
    "    * Базовая аугментация (поворот + отражение).\n",
    "    * Цветовая аугментация (яркость, контраст, HSV).\n",
    "    * Геометрическая аугментация (масштабирование, искажение).\n",
    "    * Сложная аугментация (Cutout + MixUp).\n",
    "\n",
    "Большим плюсом будет применение Pytorch Lightning для упорядочивания вашего проекта, но это не обязательное требование.\n",
    "\n",
    "\n",
    "## Критерий оценки:\n",
    "1. Есть сводная табличка в конце со сравнением стратегий.\n",
    "2. У каждой стратегии есть краткое понятное описание, что именно сделано.\n",
    "3. Есть визуализация собранного датасета.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Конфигурация\n",
    "DATA_ROOT = \"DataForModel/images\"\n",
    "TRAIN_ANN = \"DataForModel/annotations/instances_Train.json\"\n",
    "VAL_ANN = \"DataForModel/annotations/instances_Validation.json\"\n",
    "CLASSES = [\"background\", \"defect\"]\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\core\\composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n",
      "c:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 1. Определение стратегий аугментации\n",
    "AUGMENTATION_STRATEGIES = {\n",
    "    'base': A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "    \n",
    "    'color': A.Compose([\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.CLAHE(p=0.3),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "    \n",
    "    'geo': A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "        A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "    \n",
    "    'none': A.Compose([\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Датасет для дефектов ткани\n",
    "class FabricDefectDataset(Dataset):\n",
    "    def __init__(self, root, ann_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        with open(ann_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.anns = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.anns:\n",
    "                self.anns[img_id] = []\n",
    "            ann['category_id'] = 1  # Все объекты - дефекты\n",
    "            self.anns[img_id].append(ann)\n",
    "        \n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        boxes = []\n",
    "        for ann in self.anns.get(img_id, []):\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "        \n",
    "        labels = [1] * len(boxes)  # Все метки = 1 (дефект)\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=img,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            img = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Модель и обучение\n",
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, \n",
    "        num_classes=2  # background + defect\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def train_step(model, optimizer, images, targets):\n",
    "    model.train()\n",
    "    images = [image.to(DEVICE) for image in images]\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return losses.item()\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(DEVICE) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            preds = []\n",
    "            for output in outputs:\n",
    "                preds.append({\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                })\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': t['boxes'].cpu(),\n",
    "                'labels': t['labels'].cpu()\n",
    "            } for t in targets]\n",
    "            \n",
    "            metric.update(preds, targets)\n",
    "    \n",
    "    return metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (2215053725.py, line 21)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtrain_loader = DataLoader(\u001b[39m\n                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "strategies = ['base', 'color', 'geo', 'none']\n",
    "    \n",
    "for strategy in strategies:\n",
    "    print(f\"\\n=== Training with {strategy} augmentation ===\")\n",
    "    \n",
    "    # Датасеты\n",
    "    train_dataset = FabricDefectDataset(\n",
    "        root=os.path.join(DATA_ROOT, \"train\"),\n",
    "        ann_file=os.path.join(DATA_ROOT, TRAIN_ANN),\n",
    "        transforms=AUGMENTATION_STRATEGIES[strategy]\n",
    "    )\n",
    "    \n",
    "    val_dataset = FabricDefectDataset(\n",
    "        root=os.path.join(DATA_ROOT, \"val\"),\n",
    "        ann_file=os.path.join(DATA_ROOT, VAL_ANN),\n",
    "        transforms=AUGMENTATION_STRATEGIES['none']\n",
    "    )\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=True, \n",
    "        collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=2,\n",
    "        collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    # Инициализация модели\n",
    "    model = get_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Обучение\n",
    "    best_map = 0.0\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            loss = train_step(model, optimizer, images, targets)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Валидация\n",
    "        metrics = evaluate(model, val_loader)\n",
    "        current_map = metrics['map'].item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/10 | Loss: {total_loss:.2f} | mAP: {current_map:.3f}\")\n",
    "        \n",
    "        if current_map > best_map:\n",
    "            best_map = current_map\n",
    "            torch.save(model.state_dict(), f\"best_{strategy}.pth\")\n",
    "    \n",
    "    results[strategy] = best_map\n",
    "\n",
    "# Результаты\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(pd.DataFrame.from_dict(results, orient='index', columns=['mAP']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
