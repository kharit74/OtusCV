{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание:\n",
    "Собрать и подготовить свой собственный датасет, можно в соответствии с темой будущего проекта.\n",
    "Аугментировать данные техниками из занятия.\n",
    "\n",
    "## Данные:\n",
    "Свой датасет\n",
    "\n",
    "## Задачи:\n",
    "1) Взять предобученную модель. \n",
    "    * Cтандартную предобученную модель из библиотек PyTorch `torchvision.models`\n",
    "    * Примеры: ResNet18, VGG16, EfficientNet.\n",
    "2) Применить несколько стратегий аугментации данных \n",
    "    * Определение стратегий аугментации\n",
    "    * Пример Torchvision: `transforms.RandomHorizontalFlip`, `transforms.ColorJitter`.\n",
    "    * Пример Albumentations: `HorizontalFlip`, `RandomBrightnessContrast`, `Cutout`, `ShiftScaleRotate`.\n",
    "    * Пример сложных: <i>MixUp</i>, <i>CutMix</i>, <i>Mosaic</i>, <i>Weather</i>, <i>Style Transfer</i>.\n",
    "3) Собрать результаты успешности стратегий в сводную табличку.\n",
    "    * Базовая аугментация (поворот + отражение).\n",
    "    * Цветовая аугментация (яркость, контраст, HSV).\n",
    "    * Геометрическая аугментация (масштабирование, искажение).\n",
    "    * Сложная аугментация (Cutout + MixUp).\n",
    "\n",
    "Большим плюсом будет применение Pytorch Lightning для упорядочивания вашего проекта, но это не обязательное требование.\n",
    "\n",
    "\n",
    "## Критерий оценки:\n",
    "1. Есть сводная табличка в конце со сравнением стратегий.\n",
    "2. У каждой стратегии есть краткое понятное описание, что именно сделано.\n",
    "3. Есть визуализация собранного датасета.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Конфигурация\n",
    "DATA_ROOT = \"DataForModel/images\"\n",
    "TRAIN_ANN = \"DataForModel/annotations/instances_Train.json\"\n",
    "VAL_ANN = \"DataForModel/annotations/instances_Validation.json\"\n",
    "CLASSES = [\"background\", \"defect\"]\n",
    "\n",
    "# Определение путей\n",
    "IMAGES_ROOT = \"DataForModel/images\"  # Корневая директория для изображений\n",
    "ANNOTATIONS_ROOT = \"DataForModel/annotations\"  # Корневая директория для аннотаций\n",
    "\n",
    "TRAIN_IMAGES = os.path.join(IMAGES_ROOT, \"Train\")  # Путь к тренировочным изображениям\n",
    "TRAIN_ANN = os.path.join(ANNOTATIONS_ROOT, \"instances_Train.json\")  # Путь к тренировочным аннотациям\n",
    "VAL_IMAGES = os.path.join(IMAGES_ROOT, \"Validation\")  # Путь к валидационным изображениям\n",
    "VAL_ANN = os.path.join(ANNOTATIONS_ROOT, \"instances_Validation.json\")  # Путь к валидационным аннотациям\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Определение стратегий аугментации\n",
    "AUGMENTATION_STRATEGIES = {\n",
    "    'base': A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "    \n",
    "    'color': A.Compose([\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.CLAHE(p=0.3),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "    \n",
    "    'geo': A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "        A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "    \n",
    "    'none': A.Compose([\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Обновленный класс датасета с улучшенной обработкой ошибок\n",
    "class FabricDefectDataset(Dataset):\n",
    "    def __init__(self, root, ann_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        with open(ann_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Валидация данных при загрузке\n",
    "        self.images = {}\n",
    "        self.anns = {}\n",
    "        valid_ids = []\n",
    "        \n",
    "        for img in data['images']:\n",
    "            if os.path.exists(os.path.join(root, img['file_name'])):\n",
    "                self.images[img['id']] = img\n",
    "                valid_ids.append(img['id'])\n",
    "            else:\n",
    "                print(f\"Missing image: {img['file_name']}\")\n",
    "\n",
    "        for ann in data['annotations']:\n",
    "            if ann['image_id'] in valid_ids:\n",
    "                if ann['image_id'] not in self.anns:\n",
    "                    self.anns[ann['image_id']] = []\n",
    "                self.anns[ann['image_id']].append(ann)\n",
    "        \n",
    "        self.ids = valid_ids\n",
    "        print(f\"Loaded {len(self.ids)} valid images\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_id = self.ids[idx]\n",
    "            img_info = self.images[img_id]\n",
    "            img_path = os.path.join(self.root, img_info['file_name'])\n",
    "            \n",
    "            # Загрузка с проверкой изображения\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise FileNotFoundError(f\"Failed to read image: {img_path}\")\n",
    "                \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Обработка аннотаций\n",
    "            boxes = []\n",
    "            for ann in self.anns.get(img_id, []):\n",
    "                x, y, w, h = ann['bbox']\n",
    "                if w <= 0 or h <= 0:\n",
    "                    continue  # Пропустить некорректные bbox\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "            \n",
    "            labels = [1] * len(boxes)\n",
    "\n",
    "            # Применение аугментаций\n",
    "            if self.transforms:\n",
    "                transformed = self.transforms(\n",
    "                    image=img,\n",
    "                    bboxes=boxes,\n",
    "                    labels=labels\n",
    "                )\n",
    "                img = transformed['image']\n",
    "                boxes = transformed['bboxes']\n",
    "                labels = transformed['labels']\n",
    "\n",
    "            # Конвертация с проверкой пустых данных\n",
    "            if len(boxes) == 0:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            else:\n",
    "                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            \n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            # Проверка и нормализация изображения\n",
    "            if img.dim() == 2:\n",
    "                img = img.unsqueeze(0).repeat(3, 1, 1)\n",
    "            elif img.shape[0] == 1:  # Grayscale to RGB\n",
    "                img = img.repeat(3, 1, 1)\n",
    "                \n",
    "            img = img.float() / 255.0  # Явная нормализация\n",
    "\n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': labels,\n",
    "                'image_id': torch.tensor([img_id]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.zeros(0),\n",
    "                'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "            }\n",
    "\n",
    "            return img, target\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_id}: {e}\")\n",
    "            # Возвращаем пустые данные и фильтруем в collate_fn\n",
    "            return torch.zeros((3, 256, 256)), {'boxes': torch.zeros((0, 4)), 'labels': torch.zeros(0)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "# 2. Обновленная функция collate_fn с фильтрацией\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        # Фильтрация битых данных\n",
    "        if img.shape[0] != 3 or target['boxes'].shape[0] < 0:\n",
    "            continue\n",
    "            \n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        # Возвращаем пустой батч\n",
    "        return [torch.zeros((3, 256, 256))], [{'boxes': torch.zeros((0, 4)), 'labels': torch.zeros(0)}]\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Модель и обучение\n",
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, \n",
    "        num_classes=2  # background + defect\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def train_step(model, optimizer, images, targets):\n",
    "    model.train()\n",
    "    images = [image.to(DEVICE) for image in images]\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return losses.item()\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(DEVICE) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            preds = []\n",
    "            for output in outputs:\n",
    "                preds.append({\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                })\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': t['boxes'].cpu(),\n",
    "                'labels': t['labels'].cpu()\n",
    "            } for t in targets]\n",
    "            \n",
    "            metric.update(preds, targets)\n",
    "    \n",
    "    return metric.compute()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        # Нормализация изображения\n",
    "        if img.dtype == torch.uint8:\n",
    "            img = img.float() / 255.0\n",
    "        \n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with base augmentation ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m     total_loss += loss\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Валидация\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m metrics = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m current_map = metrics[\u001b[33m'\u001b[39m\u001b[33mmap\u001b[39m\u001b[33m'\u001b[39m].item()\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/10 | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | mAP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_map\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, dataloader)\u001b[39m\n\u001b[32m     27\u001b[39m metric = MeanAveragePrecision()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mFabricDefectDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     59\u001b[39m labels = torch.as_tensor(labels, dtype=torch.int64)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Проверка размерности изображения\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m() == \u001b[32m2\u001b[39m:\n\u001b[32m     63\u001b[39m     img = img.unsqueeze(\u001b[32m0\u001b[39m).repeat(\u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m img.dim() == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m img.shape[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "strategies = ['base', 'color', 'geo', 'none']\n",
    "    \n",
    "for strategy in strategies:\n",
    "    print(f\"\\n=== Training with {strategy} augmentation ===\")\n",
    "    \n",
    "    # Датасеты\n",
    "    train_dataset = FabricDefectDataset(\n",
    "        root=TRAIN_IMAGES,\n",
    "        ann_file=TRAIN_ANN,\n",
    "        transforms=AUGMENTATION_STRATEGIES[strategy]\n",
    "    )\n",
    "    \n",
    "    val_dataset = FabricDefectDataset(\n",
    "        root=VAL_IMAGES,\n",
    "        ann_file=VAL_ANN,\n",
    "        transforms=None  # На валидации аугментации не нужны\n",
    "    )\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,  # Отключаем multiprocessing для отладки\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Инициализация модели\n",
    "    model = get_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Обучение\n",
    "    best_map = 0.0\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            loss = train_step(model, optimizer, images, targets)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Валидация\n",
    "        metrics = evaluate(model, val_loader)\n",
    "        current_map = metrics['map'].item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/10 | Loss: {total_loss:.2f} | mAP: {current_map:.3f}\")\n",
    "        \n",
    "        if current_map > best_map:\n",
    "            best_map = current_map\n",
    "            torch.save(model.state_dict(), f\"best_{strategy}.pth\")\n",
    "    \n",
    "    results[strategy] = best_map\n",
    "\n",
    "# Результаты\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(pd.DataFrame.from_dict(results, orient='index', columns=['mAP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Image shape: torch.Size([3, 1280, 960])\n",
      "Boxes shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Тестирование загрузки данных\n",
    "for images, targets in train_loader:\n",
    "    print(f\"Batch size: {len(images)}\")\n",
    "    print(f\"Image shape: {images[0].shape}\")  # Должно быть [3, H, W]\n",
    "    print(f\"Boxes shape: {targets[0]['boxes'].shape}\")  # [N, 4]\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
