{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание:\n",
    "Собрать и подготовить свой собственный датасет, можно в соответствии с темой будущего проекта.\n",
    "Аугментировать данные техниками из занятия.\n",
    "\n",
    "## Данные:\n",
    "Свой датасет\n",
    "\n",
    "## Задачи:\n",
    "1) Взять предобученную модель. \n",
    "    * Cтандартную предобученную модель из библиотек PyTorch `torchvision.models`\n",
    "    * Примеры: ResNet18, VGG16, EfficientNet.\n",
    "2) Применить несколько стратегий аугментации данных \n",
    "    * Определение стратегий аугментации\n",
    "    * Пример Torchvision: `transforms.RandomHorizontalFlip`, `transforms.ColorJitter`.\n",
    "    * Пример Albumentations: `HorizontalFlip`, `RandomBrightnessContrast`, `Cutout`, `ShiftScaleRotate`.\n",
    "    * Пример сложных: <i>MixUp</i>, <i>CutMix</i>, <i>Mosaic</i>, <i>Weather</i>, <i>Style Transfer</i>.\n",
    "3) Собрать результаты успешности стратегий в сводную табличку.\n",
    "    * Базовая аугментация (поворот + отражение).\n",
    "    * Цветовая аугментация (яркость, контраст, HSV).\n",
    "    * Геометрическая аугментация (масштабирование, искажение).\n",
    "    * Сложная аугментация (Cutout + MixUp).\n",
    "\n",
    "Большим плюсом будет применение Pytorch Lightning для упорядочивания вашего проекта, но это не обязательное требование.\n",
    "\n",
    "\n",
    "## Критерий оценки:\n",
    "1. Есть сводная табличка в конце со сравнением стратегий.\n",
    "2. У каждой стратегии есть краткое понятное описание, что именно сделано.\n",
    "3. Есть визуализация собранного датасета.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#from torchvision.models.detection import FasterRCNN\n",
    "#from torchvision.models.detection import AnchorGenerator\n",
    "import albumentations as A\n",
    "from torchvision import transforms as T\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CocoDetectionDataset(Dataset):\n",
    "    # Конструктор класса\n",
    "    def __init__(self, data_dir, annotation_file, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        with open(annotation_file, 'r') as file:\n",
    "            self.coco = json.load(file)\n",
    "        \n",
    "        self.image_ids = [img['id'] for img in self.coco['images']]\n",
    "        self.imgs = {img['id']: img for img in self.coco['images']}\n",
    "        self.anns = {}\n",
    "        for ann in self.coco['annotations']:\n",
    "            if ann['image_id'] not in self.anns:\n",
    "                self.anns[ann['image_id']] = []\n",
    "            self.anns[ann['image_id']].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_info = self.imgs[image_id]\n",
    "        img_path = os.path.join(self.data_dir, img_info['file_name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        anns = self.anns.get(image_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])  # COCO format to [x_min, y_min, x_max, y_max]\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes, class_labels=labels)\n",
    "            image = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['class_labels']\n",
    "        \n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([image_id])\n",
    "        }\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовая аугментация\n",
    "basic_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.5),\n",
    "    A.ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "# Цветовая аугментация\n",
    "color_transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.HueSaturationValue(p=0.5),\n",
    "    A.ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "# Геометрическая аугментация\n",
    "geo_transform = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.5),\n",
    "    A.Perspective(p=0.3),\n",
    "    A.ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "# Сложная аугментация (Cutout + MixUp)\n",
    "# Для MixUp потребуется кастомная реализация, см. примечания ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CocoDetectionDataset.__init__() got an unexpected keyword argument 'root_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Создание датасетов и DataLoader\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_dataset = \u001b[43mCocoDetectionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLesson 9\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mHomework\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDataForModel\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mTrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLesson 9/Homework/DataForModel/annotations/instances_Train.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbasic_transform\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Меняйте трансформы здесь\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=\u001b[32m2\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*x)))\n",
      "\u001b[31mTypeError\u001b[39m: CocoDetectionDataset.__init__() got an unexpected keyword argument 'root_dir'"
     ]
    }
   ],
   "source": [
    "# Создание датасетов и DataLoader\n",
    "train_dataset = CocoDetectionDataset(\n",
    "    root_dir='Lesson 9/Homework/DataForModel/images/Train',\n",
    "    annotation_file='Lesson 9/Homework/DataForModel/annotations/instances_Train.json',\n",
    "    transforms=basic_transform  # Меняйте трансформы здесь\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     11\u001b[39m num_classes = \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# Замените на ваше количество классов\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m(num_classes)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m(num_classes):\n\u001b[32m      2\u001b[39m     backbone = torchvision.models.resnet50(pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     backbone = \u001b[43mbackbone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\n\u001b[32m      4\u001b[39m     anchor_generator = AnchorGenerator(sizes=((\u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m512\u001b[39m),),\n\u001b[32m      5\u001b[39m                                        aspect_ratios=((\u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m),))\n\u001b[32m      6\u001b[39m     model = FasterRCNN(backbone,\n\u001b[32m      7\u001b[39m                        num_classes=num_classes,\n\u001b[32m      8\u001b[39m                        rpn_anchor_generator=anchor_generator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'ResNet' object has no attribute 'backbone'"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_model(num_classes):\n",
    "    backbone = torchvision.models.resnet50(pretrained=True)\n",
    "    backbone = backbone.backbone\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    model = FasterRCNN(backbone,\n",
    "                       num_classes=num_classes,\n",
    "                       rpn_anchor_generator=anchor_generator)\n",
    "    return model\n",
    "\n",
    "num_classes = 1  # Замените на ваше количество классов\n",
    "model = get_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CocoDetectionDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.optim.Adam(\u001b[38;5;28mself\u001b[39m.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Создание датасетов и DataLoader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m train_dataset = \u001b[43mCocoDetectionDataset\u001b[49m(\n\u001b[32m     20\u001b[39m     root_dir=\u001b[33m'\u001b[39m\u001b[33mdataset/train/images\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     21\u001b[39m     annotation_file=\u001b[33m'\u001b[39m\u001b[33mdataset/train/annotations/instances_train.json\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m     transforms=basic_transform  \u001b[38;5;66;03m# Меняйте трансформы здесь\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m train_loader = DataLoader(train_dataset, batch_size=\u001b[32m2\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*x)))\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Обучение\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'CocoDetectionDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DetectionModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        loss_dict = self.model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "# Создание датасетов и DataLoader\n",
    "train_dataset = CocoDetectionDataset(\n",
    "    root_dir='dataset/train/images',\n",
    "    annotation_file='dataset/train/annotations/instances_train.json',\n",
    "    transforms=basic_transform  # Меняйте трансформы здесь\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Обучение\n",
    "model = DetectionModel(get_model(num_classes))\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=1)\n",
    "trainer.fit(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
