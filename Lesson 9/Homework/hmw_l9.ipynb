{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание:\n",
    "Собрать и подготовить свой собственный датасет, можно в соответствии с темой будущего проекта.\n",
    "Аугментировать данные техниками из занятия.\n",
    "\n",
    "## Данные:\n",
    "Свой датасет\n",
    "\n",
    "## Задачи:\n",
    "1) Взять предобученную модель. \n",
    "    * Cтандартную предобученную модель из библиотек PyTorch `torchvision.models`\n",
    "    * Примеры: ResNet18, VGG16, EfficientNet.\n",
    "2) Применить несколько стратегий аугментации данных \n",
    "    * Определение стратегий аугментации\n",
    "    * Пример Torchvision: `transforms.RandomHorizontalFlip`, `transforms.ColorJitter`.\n",
    "    * Пример Albumentations: `HorizontalFlip`, `RandomBrightnessContrast`, `Cutout`, `ShiftScaleRotate`.\n",
    "    * Пример сложных: <i>MixUp</i>, <i>CutMix</i>, <i>Mosaic</i>, <i>Weather</i>, <i>Style Transfer</i>.\n",
    "3) Собрать результаты успешности стратегий в сводную табличку.\n",
    "    * Базовая аугментация (поворот + отражение).\n",
    "    * Цветовая аугментация (яркость, контраст, HSV).\n",
    "    * Геометрическая аугментация (масштабирование, искажение).\n",
    "    * Сложная аугментация (Cutout + MixUp).\n",
    "\n",
    "Большим плюсом будет применение Pytorch Lightning для упорядочивания вашего проекта, но это не обязательное требование.\n",
    "\n",
    "\n",
    "## Критерий оценки:\n",
    "1. Есть сводная табличка в конце со сравнением стратегий.\n",
    "2. У каждой стратегии есть краткое понятное описание, что именно сделано.\n",
    "3. Есть визуализация собранного датасета.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступен: True\n",
      "Версия CUDA: 12.6\n",
      "Количество GPU: 1\n",
      "Текущее устройство: 0\n",
      "Имя GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Проверка, доступна ли CUDA\n",
    "print(\"CUDA доступен:\", torch.cuda.is_available())\n",
    "\n",
    "# Проверка версии CUDA\n",
    "print(\"Версия CUDA:\", torch.version.cuda)\n",
    "\n",
    "# Проверка количества доступных GPU\n",
    "print(\"Количество GPU:\", torch.cuda.device_count())\n",
    "\n",
    "# Проверка текущего устройства (GPU)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Текущее устройство:\", torch.cuda.current_device())\n",
    "    print(\"Имя GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"GPU не обнаружено.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Конфигурация\n",
    "CLASSES = [\"background\", \"defect\"]\n",
    "TRAIN_IMAGES = \"DataForModel/images/Train\"  # Путь к тренировочным изображениям\n",
    "TRAIN_ANN = \"DataForModel/annotations/instances_Train.json\"  # Путь к тренировочным аннотациям\n",
    "VAL_IMAGES = \"DataForModel/images/Validation\"  # Путь к валидационным изображениям\n",
    "VAL_ANN = \"DataForModel/annotations/instances_Validation.json\"  # Путь к валидационным аннотациям\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khari\\OneDrive\\Документы\\GirRep\\OtusCV\\.venv\\Lib\\site-packages\\albumentations\\core\\composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n",
      "c:\\Users\\khari\\OneDrive\\Документы\\GirRep\\OtusCV\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 1. Определение стратегий аугментации\n",
    "base = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.Rotate(limit=15, p=0.5),\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ], \n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
    ")\n",
    "\n",
    "color = A.Compose([\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.CLAHE(p=0.3),\n",
    "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ],\n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
    ")\n",
    "\n",
    "geo = A.Compose([\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "                A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ],\n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
    ")\n",
    "\n",
    "no_aug = A.Compose([\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ],\n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Обновленный класс датасета с улучшенной обработкой ошибок\n",
    "class FabricDefectDataset(Dataset):\n",
    "    def __init__(self, root, ann_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        with open(ann_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.anns = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.anns:\n",
    "                self.anns[img_id] = []\n",
    "            self.anns[img_id].append(ann)\n",
    "        \n",
    "        self.ids = list(self.images.keys())\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        boxes = []\n",
    "        for ann in self.anns.get(img_id, []):\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "        \n",
    "        labels = [1] * len(boxes)\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=img,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            img = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4))\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.zeros(0),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "# 2. Обновленная функция collate_fn с фильтрацией\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        # Фильтрация битых данных\n",
    "        if img.shape[0] != 3 or target['boxes'].shape[0] < 0:\n",
    "            continue\n",
    "            \n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        # Возвращаем пустой батч\n",
    "        return [torch.zeros((3, 256, 256))], [{'boxes': torch.zeros((0, 4)), 'labels': torch.zeros(0)}]\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Модель и обучение\n",
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, \n",
    "        num_classes=2  # background + defect\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def train_step(model, optimizer, images, targets):\n",
    "    model.train()\n",
    "    images = [image.to(DEVICE) for image in images]\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return losses.item()\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(DEVICE) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            preds = []\n",
    "            for output in outputs:\n",
    "                preds.append({\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                })\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': t['boxes'].cpu(),\n",
    "                'labels': t['labels'].cpu()\n",
    "            } for t in targets]\n",
    "            \n",
    "            metric.update(preds, targets)\n",
    "    \n",
    "    return metric.compute()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        # Нормализация изображения\n",
    "        if img.dtype == torch.uint8:\n",
    "            img = img.float() / 255.0\n",
    "        \n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with base augmentation ===\n",
      "Epoch 1/10 | Loss: 8.51 | mAP: 0.132\n",
      "Epoch 2/10 | Loss: 4.40 | mAP: 0.142\n",
      "Epoch 3/10 | Loss: 4.94 | mAP: 0.147\n",
      "Epoch 4/10 | Loss: 3.99 | mAP: 0.083\n",
      "Epoch 5/10 | Loss: 3.73 | mAP: 0.134\n",
      "Epoch 6/10 | Loss: 3.52 | mAP: 0.138\n",
      "Epoch 7/10 | Loss: 4.03 | mAP: 0.199\n",
      "Epoch 8/10 | Loss: 3.96 | mAP: 0.204\n",
      "Epoch 9/10 | Loss: 3.18 | mAP: 0.193\n",
      "Epoch 10/10 | Loss: 3.31 | mAP: 0.224\n",
      "\n",
      "=== Training with color augmentation ===\n",
      "Epoch 1/10 | Loss: 10.12 | mAP: 0.088\n",
      "Epoch 2/10 | Loss: 5.37 | mAP: 0.038\n",
      "Epoch 3/10 | Loss: 4.45 | mAP: 0.050\n",
      "Epoch 4/10 | Loss: 4.14 | mAP: 0.080\n",
      "Epoch 5/10 | Loss: 3.85 | mAP: 0.199\n",
      "Epoch 6/10 | Loss: 3.05 | mAP: 0.175\n",
      "Epoch 7/10 | Loss: 2.93 | mAP: 0.115\n",
      "Epoch 8/10 | Loss: 2.92 | mAP: 0.142\n",
      "Epoch 9/10 | Loss: 2.60 | mAP: 0.098\n",
      "Epoch 10/10 | Loss: 2.44 | mAP: 0.157\n",
      "\n",
      "=== Training with geo augmentation ===\n",
      "Epoch 1/10 | Loss: 7.74 | mAP: 0.042\n",
      "Epoch 2/10 | Loss: 4.54 | mAP: 0.057\n",
      "Epoch 3/10 | Loss: 4.87 | mAP: 0.091\n",
      "Epoch 4/10 | Loss: 3.88 | mAP: 0.139\n",
      "Epoch 5/10 | Loss: 4.03 | mAP: 0.112\n",
      "Epoch 6/10 | Loss: 4.61 | mAP: 0.124\n",
      "Epoch 7/10 | Loss: 4.18 | mAP: 0.121\n",
      "Epoch 8/10 | Loss: 3.59 | mAP: 0.159\n",
      "Epoch 9/10 | Loss: 3.29 | mAP: 0.132\n",
      "Epoch 10/10 | Loss: 3.48 | mAP: 0.157\n",
      "\n",
      "=== Training with none augmentation ===\n",
      "Epoch 1/10 | Loss: 10.27 | mAP: 0.043\n",
      "Epoch 2/10 | Loss: 5.72 | mAP: 0.101\n",
      "Epoch 3/10 | Loss: 4.09 | mAP: 0.177\n",
      "Epoch 4/10 | Loss: 4.06 | mAP: 0.176\n",
      "Epoch 5/10 | Loss: 3.33 | mAP: 0.155\n",
      "Epoch 6/10 | Loss: 2.83 | mAP: 0.245\n",
      "Epoch 7/10 | Loss: 2.55 | mAP: 0.148\n",
      "Epoch 8/10 | Loss: 2.75 | mAP: 0.212\n",
      "Epoch 9/10 | Loss: 2.57 | mAP: 0.141\n",
      "Epoch 10/10 | Loss: 2.21 | mAP: 0.136\n",
      "\n",
      "=== Final Results ===\n",
      "            mAP\n",
      "base   0.223766\n",
      "color  0.199476\n",
      "geo    0.159054\n",
      "none   0.244697\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "strategies = {\n",
    "    'base': base,\n",
    "    'color': color,\n",
    "    'geo': geo,\n",
    "    'none': no_aug\n",
    "}\n",
    "\n",
    "for strategy_name, augmentation in strategies.items():\n",
    "    print(f\"\\n=== Training with {strategy_name} augmentation ===\")\n",
    "    \n",
    "    # Датасеты\n",
    "    train_dataset = FabricDefectDataset(\n",
    "        root=TRAIN_IMAGES,\n",
    "        ann_file=TRAIN_ANN,\n",
    "        transforms=augmentation  # Передаем конкретную стратегию аугментации\n",
    "    )\n",
    "    \n",
    "    val_dataset = FabricDefectDataset(\n",
    "        root=VAL_IMAGES,\n",
    "        ann_file=VAL_ANN,\n",
    "        transforms=no_aug  # Для валидации используем базовую обработку\n",
    "    )\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Инициализация модели\n",
    "    model = get_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Обучение\n",
    "    best_map = 0.0\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            loss = train_step(model, optimizer, images, targets)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Валидация\n",
    "        metrics = evaluate(model, val_loader)\n",
    "        current_map = metrics['map'].item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/10 | Loss: {total_loss:.2f} | mAP: {current_map:.3f}\")\n",
    "        \n",
    "        if current_map > best_map:\n",
    "            best_map = current_map\n",
    "            torch.save(model.state_dict(), f\"best_{strategy_name}.pth\")\n",
    "    \n",
    "    results[strategy_name] = best_map\n",
    "\n",
    "# Результаты\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(pd.DataFrame.from_dict(results, orient='index', columns=['mAP']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
