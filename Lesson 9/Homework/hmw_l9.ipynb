{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание:\n",
    "Собрать и подготовить свой собственный датасет, можно в соответствии с темой будущего проекта.\n",
    "Аугментировать данные техниками из занятия.\n",
    "\n",
    "## Данные:\n",
    "Свой датасет\n",
    "\n",
    "## Задачи:\n",
    "1) Взять предобученную модель. \n",
    "    * Cтандартную предобученную модель из библиотек PyTorch `torchvision.models`\n",
    "    * Примеры: ResNet18, VGG16, EfficientNet.\n",
    "2) Применить несколько стратегий аугментации данных \n",
    "    * Определение стратегий аугментации\n",
    "    * Пример Torchvision: `transforms.RandomHorizontalFlip`, `transforms.ColorJitter`.\n",
    "    * Пример Albumentations: `HorizontalFlip`, `RandomBrightnessContrast`, `Cutout`, `ShiftScaleRotate`.\n",
    "    * Пример сложных: <i>MixUp</i>, <i>CutMix</i>, <i>Mosaic</i>, <i>Weather</i>, <i>Style Transfer</i>.\n",
    "3) Собрать результаты успешности стратегий в сводную табличку.\n",
    "    * Базовая аугментация (поворот + отражение).\n",
    "    * Цветовая аугментация (яркость, контраст, HSV).\n",
    "    * Геометрическая аугментация (масштабирование, искажение).\n",
    "    * Сложная аугментация (Cutout + MixUp).\n",
    "\n",
    "Большим плюсом будет применение Pytorch Lightning для упорядочивания вашего проекта, но это не обязательное требование.\n",
    "\n",
    "\n",
    "## Критерий оценки:\n",
    "1. Есть сводная табличка в конце со сравнением стратегий.\n",
    "2. У каждой стратегии есть краткое понятное описание, что именно сделано.\n",
    "3. Есть визуализация собранного датасета.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Конфигурация\n",
    "CLASSES = [\"background\", \"defect\"]\n",
    "TRAIN_IMAGES = \"DataForModel/images/Train\"  # Путь к тренировочным изображениям\n",
    "TRAIN_ANN = \"DataForModel/annotations/instances_Train.json\"  # Путь к тренировочным аннотациям\n",
    "VAL_IMAGES = \"DataForModel/images/Validation\"  # Путь к валидационным изображениям\n",
    "VAL_ANN = \"DataForModel/annotations/instances_Validation.json\"  # Путь к валидационным аннотациям\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KharitWinPC\\AppData\\Roaming\\Python\\Python311\\site-packages\\albumentations\\core\\composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n",
      "C:\\Users\\KharitWinPC\\AppData\\Roaming\\Python\\Python311\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 1. Определение стратегий аугментации\n",
    "base = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.Rotate(limit=15, p=0.5),\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ], \n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
    ")\n",
    "\n",
    "color = A.Compose([\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.CLAHE(p=0.3),\n",
    "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ],\n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
    ")\n",
    "\n",
    "geo = A.Compose([\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "                A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ],\n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])),\n",
    "\n",
    "no_aug = A.Compose([\n",
    "                A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),\n",
    "                ToTensorV2()\n",
    "            ],\n",
    "                bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Обновленный класс датасета с улучшенной обработкой ошибок\n",
    "class FabricDefectDataset(Dataset):\n",
    "    def __init__(self, root, ann_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        with open(ann_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in data['images']}\n",
    "        self.anns = {}\n",
    "        for ann in data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.anns:\n",
    "                self.anns[img_id] = []\n",
    "            self.anns[img_id].append(ann)\n",
    "        \n",
    "        self.ids = list(self.images.keys())\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        boxes = []\n",
    "        for ann in self.anns.get(img_id, []):\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "        \n",
    "        labels = [1] * len(boxes)\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=img,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            img = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4))\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.zeros(0),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "# 2. Обновленная функция collate_fn с фильтрацией\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        # Фильтрация битых данных\n",
    "        if img.shape[0] != 3 or target['boxes'].shape[0] < 0:\n",
    "            continue\n",
    "            \n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        # Возвращаем пустой батч\n",
    "        return [torch.zeros((3, 256, 256))], [{'boxes': torch.zeros((0, 4)), 'labels': torch.zeros(0)}]\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Модель и обучение\n",
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, \n",
    "        num_classes=2  # background + defect\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def train_step(model, optimizer, images, targets):\n",
    "    model.train()\n",
    "    images = [image.to(DEVICE) for image in images]\n",
    "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return losses.item()\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(DEVICE) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            preds = []\n",
    "            for output in outputs:\n",
    "                preds.append({\n",
    "                    'boxes': output['boxes'].cpu(),\n",
    "                    'scores': output['scores'].cpu(),\n",
    "                    'labels': output['labels'].cpu()\n",
    "                })\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': t['boxes'].cpu(),\n",
    "                'labels': t['labels'].cpu()\n",
    "            } for t in targets]\n",
    "            \n",
    "            metric.update(preds, targets)\n",
    "    \n",
    "    return metric.compute()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        # Нормализация изображения\n",
    "        if img.dtype == torch.uint8:\n",
    "            img = img.float() / 255.0\n",
    "        \n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Датасеты\n",
    "train_dataset = FabricDefectDataset(\n",
    "    root=TRAIN_IMAGES,\n",
    "    ann_file=TRAIN_ANN,\n",
    "    transforms=no_aug)\n",
    "\n",
    "val_dataset = FabricDefectDataset(\n",
    "    root=VAL_IMAGES,\n",
    "    ann_file=VAL_ANN,\n",
    "    transforms=no_aug)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Инициализация модели\n",
    "model = get_model()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with base augmentation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KharitWinPC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\KharitWinPC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 7.52 | mAP: 0.073\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     loss = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     total_loss += loss\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Валидация\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, optimizer, images, targets)\u001b[39m\n\u001b[32m     17\u001b[39m losses = \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict.values())\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mlosses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m optimizer.step()\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m losses.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KharitWinPC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KharitWinPC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KharitWinPC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "strategies = {\n",
    "    'base': base,\n",
    "    'color': color,\n",
    "    'geo': geo,\n",
    "    'none': no_aug\n",
    "}\n",
    "\n",
    "for strategy_name, augmentation in strategies.items():\n",
    "    print(f\"\\n=== Training with {strategy_name} augmentation ===\")\n",
    "    \n",
    "    # Датасеты\n",
    "    train_dataset = FabricDefectDataset(\n",
    "        root=TRAIN_IMAGES,\n",
    "        ann_file=TRAIN_ANN,\n",
    "        transforms=augmentation  # Передаем конкретную стратегию аугментации\n",
    "    )\n",
    "    \n",
    "    val_dataset = FabricDefectDataset(\n",
    "        root=VAL_IMAGES,\n",
    "        ann_file=VAL_ANN,\n",
    "        transforms=no_aug  # Для валидации используем базовую обработку\n",
    "    )\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Инициализация модели\n",
    "    model = get_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Обучение\n",
    "    best_map = 0.0\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            loss = train_step(model, optimizer, images, targets)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Валидация\n",
    "        metrics = evaluate(model, val_loader)\n",
    "        current_map = metrics['map'].item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/10 | Loss: {total_loss:.2f} | mAP: {current_map:.3f}\")\n",
    "        \n",
    "        if current_map > best_map:\n",
    "            best_map = current_map\n",
    "            torch.save(model.state_dict(), f\"best_{strategy_name}.pth\")\n",
    "    \n",
    "    results[strategy_name] = best_map\n",
    "\n",
    "# Результаты\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(pd.DataFrame.from_dict(results, orient='index', columns=['mAP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Image shape: torch.Size([3, 1280, 960])\n",
      "Boxes shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Тестирование загрузки данных\n",
    "for images, targets in train_loader:\n",
    "    print(f\"Batch size: {len(images)}\")\n",
    "    print(f\"Image shape: {images[0].shape}\")  # Должно быть [3, H, W]\n",
    "    print(f\"Boxes shape: {targets[0]['boxes'].shape}\")  # [N, 4]\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
